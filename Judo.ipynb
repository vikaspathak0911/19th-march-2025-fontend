{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16fa2iPS6s-5afNIf-HNRYHttDBmUvDJ2",
      "authorship_tag": "ABX9TyNbiA7qzgUylrwQonwHnjgi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikaspathak0911/19th-march-2025-fontend/blob/main/Judo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7wQgzmFXvhE",
        "outputId": "173e9629-a96b-48f8-9140-681f9cedbbb3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.146)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract # <<< ADD THIS IMPORT\n",
        "\n",
        "\n",
        "# --- 1. Load a pre-trained YOLOv8 SEGMENTATION model ---\n",
        "# IMPORTANT: Use 'yolov8n-seg.pt' (or 'yolov8s-seg.pt', etc.) for masks!\n",
        "person_segmentation_model = YOLO('yolov8n-seg.pt')\n",
        "\n",
        "# --- 2. Load your custom-trained YOLOv8 model for TIMER DETECTION ---\n",
        "# You need to train this model first and put its path here.\n",
        "# For example: model_timer = YOLO('runs/detect/train_timer_dataset/weights/best.pt')\n",
        "timer_detection_model = YOLO('/path/to/your/trained_timer_model/best.pt') # <<< REPLACE WITH YOUR PATH\n",
        "\n",
        "# --- Configure Tesseract OCR ---\n",
        "# Make sure Tesseract is installed and its path is correctly set if needed\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract' # Uncomment and set if Tesseract isn't in PATH\n",
        "OCR_CONFIG = r'--oem 3 --psm 6 outputbase digits' # Optimize for digits\n",
        "\n",
        "# --- Video Input ---\n",
        "video_path = '/content/drive/My Drive/Japan vs South Korea .mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file: {video_path}\")\n",
        "    print(\"Please check the file path and ensure the video file exists and GDrive is mounted.\")\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "xwR2KUxBYZUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1f5a7dd4-0eb7-4474-ee83-76268766d3e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.74M/6.74M [00:00<00:00, 75.7MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/your/trained_timer_model/best.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-970fe40d94fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# You need to train this model first and put its path here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# For example: model_timer = YOLO('runs/detect/train_timer_dataset/weights/best.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtimer_detection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/your/trained_timer_model/best.pt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <<< REPLACE WITH YOUR PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- Configure Tesseract OCR ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"RTDETR\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if RTDETR head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTDETR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Delete super().training for accessing self.model.training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModel\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \"\"\"\n\u001b[0;32m-> 1540\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight, safe_only)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# e.name is missing module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/patches.py\u001b[0m in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_torch_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/trained_timer_model/best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hofw1fm2y0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Frame Range to Process (Optional, for testing) ---\n",
        "start_frame_number = 900 # Example: Start from beginning\n",
        "end_frame_number = 1500 # Example: Process up to frame 500\n",
        "\n",
        "# Set the video's starting position (only if start_frame_number > 0)\n",
        "if start_frame_number > 0:\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_number)\n",
        "    print(f\"Set video playback to start from frame: {start_frame_number}\")\n",
        "\n",
        "# --- Video Output Setup ---\n",
        "output_video_path = f'/content/drive/My Drive/judo_detection_output_color_3players_frames_{start_frame_number}_to_{end_frame_number}.mp4'\n",
        "\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4 output\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# --- Define ROI (for persons, not timer if using YOLOv8 for timer) ---\n",
        "# This ROI is for limiting person detection to the center, keep it as is.\n",
        "center_roi_height_ratio = 0.7\n",
        "\n",
        "print(f\"Processing video: {video_path}\")\n",
        "print(f\"Output will be saved to: {output_video_path}\")\n",
        "print(f\"Original FPS: {fps}, Resolution: {frame_width}x{frame_height}\")\n",
        "print(f\"Processing frames from {start_frame_number} to {end_frame_number}...\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a6edZSA6roxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to determine dominant clothing color ---\n",
        "def get_dominant_color(roi_img, mask):\n",
        "    \"\"\"\n",
        "    Analyzes pixels within the mask of the ROI image to determine dominant color.\n",
        "    Assumes Judo-specific colors (white, blue).\n",
        "    \"\"\"\n",
        "    if roi_img is None or mask is None or mask.sum() == 0:\n",
        "        return \"N/A\"\n",
        "\n",
        "    # Resize mask to match the cropped image's dimensions\n",
        "    mask_resized = cv2.resize(mask, (roi_img.shape[1], roi_img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Convert mask to boolean for direct indexing (0/1 values to True/False)\n",
        "    mask_bool = mask_resized > 0\n",
        "\n",
        "    # Now, use the boolean mask to get only the pixels belonging to the person\n",
        "    person_pixels = roi_img[mask_bool]\n",
        "\n",
        "    if len(person_pixels) == 0:\n",
        "        return \"N/A\" # No pixels found within the mask\n",
        "\n",
        "    # Convert to HSV color space for better color analysis\n",
        "    # Keep the shape as (N, 1, 3) which is an image-like structure\n",
        "    hsv_pixels = cv2.cvtColor(person_pixels.astype(np.uint8).reshape(-1, 1, 3), cv2.COLOR_BGR2HSV) # No need for the second reshape\n",
        "\n",
        "    # Define common Judo gi color ranges in HSV (Hue: 0-179, Saturation/Value: 0-255)\n",
        "    # Ensure bounds are 1D arrays of dtype=np.uint8\n",
        "    white_lower = np.array([0, 0, 150], dtype=np.uint8)\n",
        "    white_upper = np.array([179, 70, 255], dtype=np.uint8)\n",
        "\n",
        "    blue_lower = np.array([90, 50, 50], dtype=np.uint8)\n",
        "    blue_upper = np.array([130, 255, 255], dtype=np.uint8)\n",
        "\n",
        "    black_lower = np.array([0, 0, 0], dtype=np.uint8)\n",
        "    black_upper = np.array([179, 255, 60], dtype=np.uint8)\n",
        "\n",
        "    # Count pixels falling into each range\n",
        "    # Pass hsv_pixels in the (N, 1, 3) format\n",
        "    white_pixels = np.sum(cv2.inRange(hsv_pixels, white_lower, white_upper))\n",
        "    blue_pixels = np.sum(cv2.inRange(hsv_pixels, blue_lower, blue_upper))\n",
        "    black_pixels = np.sum(cv2.inRange(hsv_pixels, black_lower, black_upper))\n",
        "\n",
        "    # Determine dominant color based on pixel counts\n",
        "    color_counts = {\n",
        "        \"White\": white_pixels,\n",
        "        \"Blue\": blue_pixels,\n",
        "        \"Black\": black_pixels,\n",
        "    }\n",
        "\n",
        "    filtered_counts = {k: v for k, v in color_counts.items() if v > 0}\n",
        "\n",
        "    if not filtered_counts:\n",
        "        return \"Other/Undefined\"\n",
        "\n",
        "    dominant_name = max(filtered_counts, key=filtered_counts.get)\n",
        "    return dominant_name\n",
        "\n"
      ],
      "metadata": {
        "id": "exWV_Higr3k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Timer State Variables ---\n",
        "last_timer_value = None\n",
        "timer_is_running_flag = False\n",
        "timer_detection_confidence_threshold = 0.5 # Confidence for YOLOv8 timer detection\n"
      ],
      "metadata": {
        "id": "-N-vjmzD7pxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Video Processing Loop ---\n",
        "current_frame_number = start_frame_number\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"End of video stream or error reading frame.\")\n",
        "        break\n",
        "\n",
        "    if current_frame_number >= end_frame_number:\n",
        "        print(f\"Reached end_frame_number ({end_frame_number}). Stopping.\")\n",
        "        break\n",
        "\n",
        "    if current_frame_number % int(fps) == 0:\n",
        "        print(f\"Processing frame {current_frame_number}...\")\n",
        "\n",
        "    frame_height, frame_width, _ = frame.shape\n",
        "\n",
        "    # --- TIMER DETECTION LOGIC ---\n",
        "    current_timer_value = None\n",
        "    timer_detected_in_frame = False\n",
        "\n",
        "    # Run inference for timer detection\n",
        "    timer_results = timer_detection_model(frame, conf=timer_detection_confidence_threshold, verbose=False)\n",
        "\n",
        "    if timer_results and timer_results[0].boxes and len(timer_results[0].boxes) > 0:\n",
        "        # Assuming only one main timer, take the first detection (highest confidence)\n",
        "        timer_box_data = timer_results[0].boxes[0]\n",
        "        timer_confidence = timer_box_data.conf.cpu().numpy()[0] # Get confidence of the top timer detection\n",
        "\n",
        "        if timer_confidence > timer_detection_confidence_threshold: # Check confidence\n",
        "            tx1, ty1, tx2, ty2 = map(int, timer_box_data.xyxy[0].cpu().numpy())\n",
        "\n",
        "            # Ensure the bounding box is within frame boundaries\n",
        "            tx1, ty1, tx2, ty2 = max(0, tx1), max(0, ty1), min(frame.shape[1], tx2), min(frame.shape[0], ty2)\n",
        "\n",
        "            timer_region = frame[ty1:ty2, tx1:tx2]\n",
        "\n",
        "            if timer_region.size > 0: # Ensure region is not empty\n",
        "                # Preprocessing for OCR\n",
        "                gray_timer_region = cv2.cvtColor(timer_region, cv2.COLOR_BGR2GRAY)\n",
        "                # Apply appropriate thresholding (Otsu is a good start)\n",
        "                _, thresh_timer_region = cv2.threshold(gray_timer_region, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "                # Further noise reduction/morphological operations might be needed depending on timer quality\n",
        "                # For example:\n",
        "                # kernel = np.ones((1,1), np.uint8)\n",
        "                # thresh_timer_region = cv2.erode(thresh_timer_region, kernel, iterations=1)\n",
        "                # thresh_timer_region = cv2.dilate(thresh_timer_region, kernel, iterations=1)\n",
        "\n",
        "                try:\n",
        "                    # Use Tesseract to read digits\n",
        "                    text = pytesseract.image_to_string(thresh_timer_region, config=OCR_CONFIG)\n",
        "\n",
        "                    # Clean and convert to integer (e.g., \"01:23\" -> 123, or just \"123\")\n",
        "                    # You might need more sophisticated parsing depending on your timer format (e.g., MM:SS)\n",
        "                    cleaned_text = ''.join(filter(str.isdigit, text))\n",
        "\n",
        "                    if cleaned_text:\n",
        "                        # If timer is MM:SS, convert to total seconds for comparison\n",
        "                        if ':' in text: # Simple check for MM:SS format\n",
        "                            parts = text.split(':')\n",
        "                            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():\n",
        "                                current_timer_value = int(parts[0]) * 60 + int(parts[1])\n",
        "                            else:\n",
        "                                current_timer_value = int(cleaned_text) # Fallback if parsing fails\n",
        "                        else:\n",
        "                            current_timer_value = int(cleaned_text)\n",
        "\n",
        "                        timer_detected_in_frame = True\n",
        "                except Exception as e:\n",
        "                    # print(f\"OCR Error on timer region: {e}\") # Uncomment for debugging OCR issues\n",
        "                    pass # Keep current_timer_value as None if OCR fails\n",
        "            # else:\n",
        "            #     print(\"Detected timer region is empty after cropping, skipping OCR.\")\n",
        "\n",
        "    # Determine if timer is running based on value change\n",
        "    if timer_detected_in_frame and last_timer_value is not None:\n",
        "        # Check for any change in value to indicate it's running\n",
        "        # You might want a more specific condition, e.g., current_timer_value < last_timer_value\n",
        "        # if timer is counting down, or current_timer_value > last_timer_value if counting up.\n",
        "        if abs(current_timer_value - last_timer_value) > 0:\n",
        "            timer_is_running_flag = True\n",
        "        else:\n",
        "            timer_is_running_flag = False # Timer is static or paused\n",
        "    elif timer_detected_in_frame and last_timer_value is None:\n",
        "         # First frame with timer detected, can't determine running yet\n",
        "         timer_is_running_flag = False\n",
        "    else:\n",
        "        timer_is_running_flag = False # No timer detected or OCR failed for multiple frames\n",
        "\n",
        "    last_timer_value = current_timer_value # Update for next frame\n",
        "\n",
        "    # --- Draw Timer Info for Visualization ---\n",
        "    if timer_detected_in_frame:\n",
        "        cv2.rectangle(frame, (tx1, ty1), (tx2, ty2), (255, 165, 0), 2) # Orange box for timer\n",
        "        timer_status_text = f\"Timer: {current_timer_value if current_timer_value is not None else 'N/A'}\"\n",
        "        cv2.putText(frame, timer_status_text, (tx1, ty1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 165, 0), 2)\n",
        "        cv2.putText(frame, f\"Running: {timer_is_running_flag}\", (tx1, ty1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 165, 0), 2)\n",
        "\n",
        "\n",
        "    # --- CONDITIONAL CLOTH COLOR DETECTION ---\n",
        "    if timer_is_running_flag:\n",
        "        print(f\"Timer is running ({current_timer_value}). Proceeding with cloth color detection.\")\n",
        "\n",
        "        # Calculate the coordinates for the dynamic center ROI\n",
        "        roi_w = int(frame_width * center_roi_width_ratio)\n",
        "        roi_h = int(frame_height * center_roi_height_ratio)\n",
        "        roi_x1 = (frame_width - roi_w) // 2\n",
        "        roi_y1 = (frame_height - roi_h) // 2\n",
        "        roi_x2 = roi_x1 + roi_w\n",
        "        roi_y2 = roi_y1 + roi_h\n",
        "\n",
        "        cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), (255, 0, 0), 2)  # Draw person ROI\n",
        "\n",
        "\n",
        "        # Use the person segmentation model\n",
        "\n",
        "        results = model.predict(frame, conf=0.5, classes=[0], verbose=False)\n",
        "\n",
        "        person_detections = []\n",
        "\n",
        "        for r in results:\n",
        "            if r.boxes is not None and len(r.boxes) > 0 and r.masks is not None and len(r.masks) > 0:\n",
        "                boxes = r.boxes.xyxy.cpu().numpy()\n",
        "                confidences = r.boxes.conf.cpu().numpy()\n",
        "                class_ids = r.boxes.cls.cpu().numpy()\n",
        "                masks = r.masks.data.cpu().numpy()\n",
        "\n",
        "                for i in range(len(boxes)):\n",
        "                    x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                    confidence = confidences[i]\n",
        "                    class_id = int(class_ids[i])\n",
        "\n",
        "                    if class_id == 0: # It's a person\n",
        "                        center_x = (x1 + x2) // 2\n",
        "                        center_y = (y1 + y2) // 2\n",
        "                        bbox_area = (x2 - x1) * (y2 - y1)\n",
        "                        is_in_roi = (roi_x1 <= center_x <= roi_x2 and\n",
        "                                    roi_y1 <= center_y <= roi_y2)\n",
        "\n",
        "                        if is_in_roi:\n",
        "                            person_detections.append({\n",
        "                                'box': (x1, y1, x2, y2),\n",
        "                                'confidence': confidence,\n",
        "                                'center': (center_x, center_y),\n",
        "                                'area': bbox_area,\n",
        "                                'mask': masks[i]\n",
        "                            })\n",
        "\n",
        "        selected_players = []\n",
        "        num_players_to_detect = 3\n",
        "\n",
        "        if len(person_detections) > 0:\n",
        "            person_detections.sort(key=lambda p: p['area'], reverse=True)\n",
        "            selected_players = person_detections[:num_players_to_detect]\n",
        "\n",
        "\n",
        "        for player in selected_players:\n",
        "            x1, y1, x2, y2 = player['box']\n",
        "            confidence = player['confidence']\n",
        "            player_mask = player['mask']\n",
        "\n",
        "            crop_x1 = max(0, x1)\n",
        "            crop_y1 = max(0, y1)\n",
        "            crop_x2 = min(frame_width, x2)\n",
        "            crop_y2 = min(frame_height, y2)\n",
        "\n",
        "            cropped_frame_for_color = frame[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "\n",
        "            dominant_color = get_dominant_color(cropped_frame_for_color, player_mask)\n",
        "\n",
        "            if dominant_color == \"Blue\":\n",
        "                bbox_draw_color = (255, 0, 0)\n",
        "            elif dominant_color == \"White\":\n",
        "                bbox_draw_color = (0, 255, 255)\n",
        "            elif dominant_color == \"Black\":\n",
        "                bbox_draw_color = (0, 0, 0)\n",
        "            else:\n",
        "                bbox_draw_color = (0, 0, 255)\n",
        "\n",
        "            line_thickness = 3\n",
        "\n",
        "            label = f\"Player ({dominant_color}): {confidence:.2f}\"\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), bbox_draw_color, line_thickness)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, bbox_draw_color, 2)\n",
        "\n",
        "     else:\n",
        "        print(\"Timer is NOT running. Skipping person and cloth color detection.\")\n",
        "        # You could display a message on the frame indicating no detection\n",
        "        cv2.putText(frame, \"Timer Not Running\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    current_frame_number += 1\n",
        "\n",
        "# --- Release resources ---\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"\\nFinished processing. Output video saved to: {output_video_path}\")\n"
      ],
      "metadata": {
        "id": "Ze6IFKAor-iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rq6pg83TkA-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8j95H-1P1_e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiIVTq2j1_hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4u5QklL1_jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eiYbvOZ1_mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSw8HUhz1_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HS2mbs9I1_qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LH7dE361_tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73fMGShR1_vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCrQfS6v1_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U71RAT571_zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DWnxKxBW1_1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hI3_PAT01_4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}